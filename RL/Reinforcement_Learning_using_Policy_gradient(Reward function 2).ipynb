{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Learning using Policy gradient.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO/GbXSFx48RrIQ1xveRLcB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soham-chitnis10/SAiDL-assignment/blob/main/RL/Reinforcement_Learning_using_Policy_gradient(Reward%20function%202).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wO6dDnVbe07"
      },
      "source": [
        "from gym import Env\n",
        "from gym.utils import seeding\n",
        "from gym.spaces import Box\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "\n",
        "\n",
        "class QuadraticEnv(Env):\n",
        "    def __init__(self):\n",
        "        self.seed()\n",
        "        self.a = random.uniform(0,10)\n",
        "        self.b = random.uniform(0,10)\n",
        "        self.c = random.uniform(-10,10)\n",
        "        self.d = random.uniform(-10,10)\n",
        "        self.e = random.uniform(-10,10)\n",
        "        self.f = random.uniform(-10,10)\n",
        "        self.x = random.uniform(-4,4)\n",
        "        self.y = random.uniform(-4,4)\n",
        "        self.state = np.array([self.x,self.y,self.a,self.b,self.c,self.d,self.e,self.f])\n",
        "        self.action_space = Box(low=-1, high=1, shape=(2,))\n",
        "        self.low_state = np.array([random.uniform(-4,4)-5,random.uniform(-4,4)-5,self.a,self.b,self.c,self.d,self.e,self.f],dtype=np.float32)\n",
        "        self.high_state = np.array([random.uniform(-4,4)+5,random.uniform(-4,4)+5,self.a,self.b,self.c,self.d,self.e,self.f],dtype=np.float32)\n",
        "        self.observation_space = Box(low=self.low_state, high=self.high_state)\n",
        "        self.set_minima()\n",
        "    def set_minima(self):\n",
        "\n",
        "        det = 4 * self.a * self.b - self.c * self.c\n",
        "        while det == 0:\n",
        "            self.reset()\n",
        "            det = 4 * self.a * self.b - self.c * self.c\n",
        "        \n",
        "        self.x_min = (-2 * self.b * self.d + self.c * self.e)/det\n",
        "        self.y_min = (self.c * self.d - 2 * self.a * self.e)/det   \n",
        "    def seed(self,seed = None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "    def step(self,action):\n",
        "        done = False\n",
        "        reward = self.reward_new(action)\n",
        "        self.state[0]-= action[0][0]\n",
        "        self.state[1]-= action[0][1]\n",
        "        if abs(self.state[0]-self.x_min)<0.1 and abs(self.state[1]-self.y_min)<0.1 :\n",
        "            done = True\n",
        "        elif (self.end - self.start) >= 60:\n",
        "            done = True\n",
        "\n",
        "        return self.state,reward,done\n",
        "    def render(self):\n",
        "        pass\n",
        "    def reset(self):\n",
        "        self.a = random.uniform(0,10)\n",
        "        self.b = random.uniform(0,10)\n",
        "        self.c = random.uniform(-10,10)\n",
        "        self.d = random.uniform(-10,10)\n",
        "        self.e = random.uniform(-10,10)\n",
        "        self.f = random.uniform(-10,10)\n",
        "        self.set_minima()\n",
        "        self.state = np.array([random.uniform(-4,4),random.uniform(-4,4),self.a,self.b,self.c,self.d,self.e,self.f])\n",
        "        return self.state\n",
        "    def reward(self):\n",
        "        state = self.state\n",
        "        dist = np.sqrt((state[0]-self.x_min)**2 + (state[1]-self.y_min)**2)\n",
        "        reward = 1/dist\n",
        "        return reward\n",
        "    def reward_new(self,action):\n",
        "        reward = np.exp(action[0][0]/(-(self.x_min - self.state[0]) - 1e-9)) + np.exp(action[0][1]/(-(self.y_min - self.state[1]) - 1e-9))\n",
        "        return reward\n",
        "    def start_time(self):\n",
        "        self.start = time.time()\n",
        "    def end_time(self):\n",
        "        self.end = time.time()\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjEBPll9bjOM"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.autograd as autograd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "\n",
        "LOG_SIG_MAX = 2\n",
        "LOG_SIG_MIN = -20\n",
        "\n",
        "\n",
        "\n",
        "class Gaussian_Policy(nn.Module):\n",
        "    '''\n",
        "    Gaussian policy that consists of a neural network with 1 hidden layer that\n",
        "    outputs mean and log std dev (the params) of a gaussian policy\n",
        "    '''\n",
        "\n",
        "    def __init__(self, num_inputs, hidden_size, action_space):\n",
        "\n",
        "       \tsuper(Gaussian_Policy, self).__init__()\n",
        "\n",
        "        self.action_space = action_space\n",
        "        num_outputs = action_space.shape[0] # the number of output actions\n",
        "        self.linear = nn.Linear(num_inputs, hidden_size)\n",
        "        self.mean = nn.Linear(hidden_size, num_outputs)\n",
        "        self.log_std = nn.Linear(hidden_size, num_outputs)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        # forward pass of NN\n",
        "        x = inputs\n",
        "        x = F.relu(self.linear(x))\n",
        "\n",
        "        mean = self.mean(x)\n",
        "        log_std = self.log_std(x) # if more than one action this will give you the diagonal elements of a diagonal covariance matrix\n",
        "        log_std = torch.clamp(log_std, min=LOG_SIG_MIN, max=LOG_SIG_MAX) # We limit the variance by forcing within a range of -2,20 as policy gradient has tendency of high variance\n",
        "        std = log_std.exp()\n",
        "\n",
        "        return mean, std"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WINvZ2SEboCP"
      },
      "source": [
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.autograd as autograd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "\n",
        "\n",
        "\n",
        "class REINFORCE:\n",
        "    '''\n",
        "    Implementation of the basic online reinforce algorithm for Gaussian policies.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, num_inputs, hidden_size, action_space, lr_pi = 1e-3,gamma = 0.99):\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.action_space = action_space\n",
        "        self.policy = Gaussian_Policy(num_inputs, hidden_size, action_space)\n",
        "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr = lr_pi)\n",
        "\n",
        "\n",
        "    def select_action(self,state):\n",
        "\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0) # just to make it a Tensor obj\n",
        "        # get mean and std\n",
        "        mean, std = self.policy.forward(state)\n",
        "\n",
        "        # create normal distribution\n",
        "        normal = Normal(mean, std)\n",
        "\n",
        "        # sample action\n",
        "        action = normal.sample()\n",
        "\n",
        "        # get log prob of that action\n",
        "        ln_prob = normal.log_prob(action)\n",
        "        ln_prob = ln_prob.sum()\n",
        "\t# squeeze action into [-1,1]\n",
        "        action = torch.tanh(action)\n",
        "        # turn actions into numpy array\n",
        "        action = action.numpy()\n",
        "\n",
        "        return action, ln_prob #, mean, std\n",
        "\n",
        "    def train(self, trajectory):\n",
        "\n",
        "        '''\n",
        "        The training is done using the rewards-to-go formulation of the policy gradient update of Reinforce.\n",
        "        trajectory: a list of the form [( state , action , lnP(a_t|s_t), reward ), ...  ]\n",
        "        '''\n",
        "\n",
        "        log_probs = [item[2] for item in trajectory]\n",
        "        rewards = [item[3] for item in trajectory]\n",
        "        states = [item[0] for item in trajectory]\n",
        "        actions = [item[1] for item in trajectory]\n",
        "\n",
        "\t#calculate rewards to go\n",
        "        R = 0\n",
        "        returns = []\n",
        "        for r in rewards[::-1]:\n",
        "            R = r + self.gamma * R\n",
        "            returns.insert(0, R)\n",
        "\n",
        "        returns = torch.tensor(returns).cuda()\n",
        "\n",
        "        policy_loss = []\n",
        "        for log_prob, R in zip(log_probs, returns):\n",
        "            log_prob = log_prob.cuda()\n",
        "            policy_loss.append( - log_prob * R)\n",
        "\n",
        "\n",
        "        policy_loss = torch.stack( policy_loss ).sum()\n",
        "        # update policy weights\n",
        "        self.policy_optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.policy_optimizer.step()\n",
        "        torch.cuda.empty_cache()\n",
        "        return policy_loss"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "MtWlzEbcbvDV",
        "outputId": "cce3467f-52b1-4620-af8d-8f2726de80c8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.autograd as autograd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "import gym\n",
        "\n",
        "def main():\n",
        "\n",
        "    # create env\n",
        "    env = QuadraticEnv()\n",
        "    env.seed(456)\n",
        "    torch.manual_seed(456)\n",
        "    np.random.seed(456)\n",
        "\n",
        "    hidden_size = 32\n",
        "\n",
        "    # get env info\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space\n",
        "\n",
        "    print(\"number of actions:{0}, dim of states: {1}\".format(action_dim,state_dim))\n",
        "\n",
        "    # create policy\n",
        "    policy = REINFORCE(state_dim, hidden_size, action_dim)\n",
        "\n",
        "    \n",
        "\n",
        "    # start of experiment: Keep looping until desired amount of episodes reached\n",
        "    max_episodes = 10\n",
        "    total_episodes = 0 # keep track of amount of episodes that we have done\n",
        "    max_reward = 0\n",
        "    max_reward_ep = 0\n",
        "    reward_list = []\n",
        "    while total_episodes < max_episodes:\n",
        "\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        trajectory = [] # trajectory info for reinforce update\n",
        "        episode_reward = 0 # keep track of rewards per episode\n",
        "        env.start_time()\n",
        "        env.end_time()\n",
        "\n",
        "        while not done:\n",
        "            action, ln_prob = policy.select_action(np.array(obs))\n",
        "            next_state, reward, done = env.step(action)\n",
        "            trajectory.append([obs, action, ln_prob, reward, next_state, done])\n",
        "            obs = next_state\n",
        "            episode_reward += reward\n",
        "            env.end_time()\n",
        "        reward_list.append(episode_reward)\n",
        "        print(f'Episode: {total_episodes} Reward: {episode_reward} function: {env.a}x^2 + {env.b}y^2 + {env.c}xy + {env.d}x + {env.e}y + {env.f} x:{env.state[0]} y:{env.state[1]} x_min:{env.x_min} y_min:{env.y_min}')\n",
        "        if episode_reward > max_reward:\n",
        "            max_reward = episode_reward\n",
        "            max_reward_ep = total_episodes\n",
        "\n",
        "        total_episodes += 1\n",
        "        policy_loss = policy.train(trajectory)\n",
        "    \n",
        "    print(f'Max Reward is {max_reward} occured on episode {max_reward_ep}')\n",
        "    eps = [ep for ep in range(1,max_episodes+1)]\n",
        "    plt.plot(eps,reward_list)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    main()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of actions:Box(-1.0, 1.0, (2,), float32), dim of states: 8\n",
            "Episode: 0 Reward: 393387.7389718114 function: 6.245151634717994x^2 + 4.742697752061892y^2 + 1.7546969434634345xy + 3.7436644687073812x + 0.12704972695595984y + 6.820686566009737 x:-147524.3829663939 y:-196684.1427802099 x_min:-0.3057909277071613 y_min:0.04317381168266571\n",
            "Episode: 1 Reward: 404798.1138675753 function: 5.102738139861001x^2 + 1.0950467396904195y^2 + 6.652742608452751xy + -0.25644049987258377x + 1.7355877394301107y + -8.638358666083581 x:-139515.67713844636 y:-202403.781095924 x_min:-0.5526760376996002 y_min:0.8863656750366937\n",
            "Episode: 2 Reward: 48811847377.93305 function: 7.3847220949888985x^2 + 7.207878784800671y^2 + -9.920857828140514xy + -5.855397845018735x + -9.607387543253726y + 1.9015309401351583 x:-130269.30073230859 y:-203939.83417642422 x_min:1.5697843466197046 y_min:1.7467687525494804\n",
            "Episode: 3 Reward: 395026.0407594231 function: 7.396552158367106x^2 + 7.315658472053555y^2 + 6.646655588934166xy + -8.918139439299566x + -4.024130763356504y + 1.302032037036172 x:-116789.57644280128 y:-197526.7603075079 x_min:0.6021964648253427 y_min:0.0014720660396476473\n",
            "Episode: 4 Reward: 406420.5310033081 function: 7.520031866307262x^2 + 4.3397980062611y^2 + 3.893065522387891xy + -7.856026658500655x + 5.521772839786042y + 1.0498397041852137 x:-123622.5438142326 y:-203203.01293532233 x_min:0.7772518353898059 y_min:-0.9847998858498647\n",
            "Episode: 5 Reward: 409106.4678333423 function: 0.6981923678874202x^2 + 7.62524877777474y^2 + 5.505743176740729xy + -2.6757148579665397x + -2.114435791797586y + -1.8609979166980963 x:-129736.17668702576 y:-204428.96511889726 x_min:-3.2341525380414806 y_min:1.3062425660603587\n",
            "Episode: 6 Reward: 404001.59691830777 function: 7.943523894086594x^2 + 9.92069889062585y^2 + 9.115610563723987xy + -7.660887429083322x + -8.141712914050434y + 3.5265705450500757 x:-132917.2369288346 y:-202006.82835169783 x_min:0.3351013506006421 y_min:0.256386145708134\n",
            "Episode: 7 Reward: 389659.2107096037 function: 1.8555166981645166x^2 + 0.41935967890383785y^2 + -1.4084252209362411xy + -1.7500683798248051x + -5.410646706409141y + 3.486395245342795 x:-131513.57210140082 y:-194609.90938703573 x_min:8.050915318861913 y_min:19.970635871453243\n",
            "Episode: 8 Reward: 410609.50411934376 function: 3.939651069091709x^2 + 2.562381215211377y^2 + 4.071677697059091xy + -7.9706995849981155x + -8.636304730762781y + -3.697051000785252 x:-143330.49425272105 y:-205317.67545135148 x_min:0.23880064943612256 y_min:1.495481118678564\n",
            "Episode: 9 Reward: 412300.89037927473 function: 5.790457251965514x^2 + 2.0857111988174415y^2 + -0.9782297016834374xy + -2.652544099835379x + -9.024148346863377y + 3.614704128997065 x:-145413.6182088099 y:-206127.88747866775 x_min:0.42010034726807627 y_min:2.2618431040687845\n",
            "Max Reward is 48811847377.93305 occured on episode 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSOngn6BflcJ"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    }
  ]
}