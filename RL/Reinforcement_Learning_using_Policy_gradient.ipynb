{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Learning using Policy gradient.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyONKPjX9RloR7KkhvH7R2oH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soham-chitnis10/SAiDL-assignment/blob/main/RL/Reinforcement_Learning_using_Policy_gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wO6dDnVbe07"
      },
      "source": [
        "from gym import Env\n",
        "from gym.utils import seeding\n",
        "from gym.spaces import Box\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "\n",
        "\n",
        "class QuadraticEnv(Env):\n",
        "    def __init__(self):\n",
        "        self.seed()\n",
        "        self.a = random.uniform(0,10)\n",
        "        self.b = random.uniform(0,10)\n",
        "        self.c = random.uniform(-10,10)\n",
        "        self.d = random.uniform(-10,10)\n",
        "        self.e = random.uniform(-10,10)\n",
        "        self.f = random.uniform(-10,10)\n",
        "        self.x = random.uniform(-4,4)\n",
        "        self.y = random.uniform(-4,4)\n",
        "        self.state = np.array([self.x,self.y,self.a,self.b,self.c,self.d,self.e,self.f])\n",
        "        self.action_space = Box(low=-1, high=1, shape=(2,))\n",
        "        self.low_state = np.array([random.uniform(-4,4)-5,random.uniform(-4,4)-5,self.a,self.b,self.c,self.d,self.e,self.f],dtype=np.float32)\n",
        "        self.high_state = np.array([random.uniform(-4,4)+5,random.uniform(-4,4)+5,self.a,self.b,self.c,self.d,self.e,self.f],dtype=np.float32)\n",
        "        self.observation_space = Box(low=self.low_state, high=self.high_state)\n",
        "        self.set_minima()\n",
        "    def set_minima(self):\n",
        "\n",
        "        det = 4 * self.a * self.b - self.c * self.c\n",
        "        while det == 0:\n",
        "            self.reset()\n",
        "            det = 4 * self.a * self.b - self.c * self.c\n",
        "        \n",
        "        self.x_min = (-2 * self.b * self.d + self.c * self.e)/det\n",
        "        self.y_min = (self.c * self.d - 2 * self.a * self.e)/det   \n",
        "    def seed(self,seed = None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "    def step(self,action):\n",
        "        done = False\n",
        "        self.state[0]-= action[0][0]\n",
        "        self.state[1]-= action[0][1]\n",
        "        reward = self.reward()\n",
        "        if abs(self.state[0]-self.x_min)<0.1 and abs(self.state[1]-self.y_min)<0.1 :\n",
        "            done = True\n",
        "        elif (self.end - self.start) >= 60:\n",
        "            done = True\n",
        "\n",
        "        return self.state,reward,done\n",
        "    def render(self):\n",
        "        pass\n",
        "    def reset(self):\n",
        "        self.a = random.uniform(0,10)\n",
        "        self.b = random.uniform(0,10)\n",
        "        self.c = random.uniform(-10,10)\n",
        "        self.d = random.uniform(-10,10)\n",
        "        self.e = random.uniform(-10,10)\n",
        "        self.f = random.uniform(-10,10)\n",
        "        self.set_minima()\n",
        "        self.state = np.array([random.uniform(-4,4),random.uniform(-4,4),self.a,self.b,self.c,self.d,self.e,self.f])\n",
        "        return self.state\n",
        "    def reward(self):\n",
        "        state = self.state\n",
        "        dist = np.sqrt((state[0]-self.x_min)**2 + (state[1]-self.y_min)**2)\n",
        "        reward = 1/dist\n",
        "        return reward\n",
        "    def start_time(self):\n",
        "        self.start = time.time()\n",
        "    def end_time(self):\n",
        "        self.end = time.time()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjEBPll9bjOM"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.autograd as autograd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "\n",
        "LOG_SIG_MAX = 2\n",
        "LOG_SIG_MIN = -20\n",
        "\n",
        "\n",
        "\n",
        "class Gaussian_Policy(nn.Module):\n",
        "    '''\n",
        "    Gaussian policy that consists of a neural network with 1 hidden layer that\n",
        "    outputs mean and log std dev (the params) of a gaussian policy\n",
        "    '''\n",
        "\n",
        "    def __init__(self, num_inputs, hidden_size, action_space):\n",
        "\n",
        "       \tsuper(Gaussian_Policy, self).__init__()\n",
        "\n",
        "        self.action_space = action_space\n",
        "        num_outputs = action_space.shape[0] # the number of output actions\n",
        "        self.linear = nn.Linear(num_inputs, hidden_size)\n",
        "        self.mean = nn.Linear(hidden_size, num_outputs)\n",
        "        self.log_std = nn.Linear(hidden_size, num_outputs)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        # forward pass of NN\n",
        "        x = inputs\n",
        "        x = F.relu(self.linear(x))\n",
        "\n",
        "        mean = self.mean(x)\n",
        "        log_std = self.log_std(x) # if more than one action this will give you the diagonal elements of a diagonal covariance matrix\n",
        "        log_std = torch.clamp(log_std, min=LOG_SIG_MIN, max=LOG_SIG_MAX) # We limit the variance by forcing within a range of -2,20 as policy gradient has tendency of high variance\n",
        "        std = log_std.exp()\n",
        "\n",
        "        return mean, std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WINvZ2SEboCP"
      },
      "source": [
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.autograd as autograd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "\n",
        "\n",
        "\n",
        "class REINFORCE:\n",
        "    '''\n",
        "    Implementation of the basic online reinforce algorithm for Gaussian policies.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, num_inputs, hidden_size, action_space, lr_pi = 3e-4,gamma = 0.99):\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.action_space = action_space\n",
        "        self.policy = Gaussian_Policy(num_inputs, hidden_size, action_space)\n",
        "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr = lr_pi)\n",
        "\n",
        "\n",
        "    def select_action(self,state):\n",
        "\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0) # just to make it a Tensor obj\n",
        "        # get mean and std\n",
        "        mean, std = self.policy.forward(state)\n",
        "\n",
        "        # create normal distribution\n",
        "        normal = Normal(mean, std)\n",
        "\n",
        "        # sample action\n",
        "        action = normal.sample()\n",
        "\n",
        "        # get log prob of that action\n",
        "        ln_prob = normal.log_prob(action)\n",
        "        ln_prob = ln_prob.sum()\n",
        "\t# squeeze action into [-1,1]\n",
        "        action = torch.tanh(action)\n",
        "        # turn actions into numpy array\n",
        "        action = action.numpy()\n",
        "\n",
        "        return action, ln_prob #, mean, std\n",
        "\n",
        "    def train(self, trajectory):\n",
        "\n",
        "        '''\n",
        "        The training is done using the rewards-to-go formulation of the policy gradient update of Reinforce.\n",
        "        trajectory: a list of the form [( state , action , lnP(a_t|s_t), reward ), ...  ]\n",
        "        '''\n",
        "\n",
        "        log_probs = [item[2] for item in trajectory]\n",
        "        rewards = [item[3] for item in trajectory]\n",
        "        states = [item[0] for item in trajectory]\n",
        "        actions = [item[1] for item in trajectory]\n",
        "\n",
        "\t#calculate rewards to go\n",
        "        R = 0\n",
        "        returns = []\n",
        "        for r in rewards[::-1]:\n",
        "            R = r + self.gamma * R\n",
        "            returns.insert(0, R)\n",
        "\n",
        "        returns = torch.tensor(returns).cuda()\n",
        "\n",
        "        policy_loss = []\n",
        "        for log_prob, R in zip(log_probs, returns):\n",
        "            log_prob = log_prob.cuda()\n",
        "            policy_loss.append( - log_prob * R)\n",
        "\n",
        "\n",
        "        policy_loss = torch.stack( policy_loss ).sum()\n",
        "        # update policy weights\n",
        "        self.policy_optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.policy_optimizer.step()\n",
        "        torch.cuda.empty_cache()\n",
        "        return policy_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtWlzEbcbvDV",
        "outputId": "b0c3f83b-698b-4e18-a6e5-d63d6016667a"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.autograd as autograd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "import gym\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    # create env\n",
        "    env = QuadraticEnv()\n",
        "    env.seed(456)\n",
        "    torch.manual_seed(456)\n",
        "    np.random.seed(456)\n",
        "\n",
        "    hidden_size = 16\n",
        "\n",
        "    # get env info\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space\n",
        "\n",
        "    print(\"number of actions:{0}, dim of states: {1}\".format(action_dim,state_dim))\n",
        "\n",
        "    # create policy\n",
        "    policy = REINFORCE(state_dim, hidden_size, action_dim)\n",
        "\n",
        "    \n",
        "\n",
        "    # start of experiment: Keep looping until desired amount of episodes reached\n",
        "    max_episodes = 10\n",
        "    total_episodes = 0 # keep track of amount of episodes that we have done\n",
        "    max_reward = 0\n",
        "    max_reward_ep = 0\n",
        "    while total_episodes < max_episodes:\n",
        "\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        trajectory = [] # trajectory info for reinforce update\n",
        "        episode_reward = 0 # keep track of rewards per episode\n",
        "        env.start_time()\n",
        "        env.end_time()\n",
        "\n",
        "        while not done:\n",
        "            action, ln_prob = policy.select_action(np.array(obs))\n",
        "            next_state, reward, done = env.step(action)\n",
        "            trajectory.append([obs, action, ln_prob, reward, next_state, done])\n",
        "            obs = next_state\n",
        "            episode_reward += reward\n",
        "            env.end_time()\n",
        "        print(f'Episode: {total_episodes} Reward: {episode_reward} function: {env.a}x^2 + {env.b}y^2 + {env.c}xy + {env.d}x + {env.e}y + {env.f} x:{env.state[0]} y:{env.state[1]} x_min:{env.x_min} y_min:{env.y_min}')\n",
        "        if episode_reward > max_reward:\n",
        "            max_reward = episode_reward\n",
        "            max_reward_ep = total_episodes\n",
        "\n",
        "        total_episodes += 1\n",
        "        policy_loss = policy.train(trajectory)\n",
        "    \n",
        "    print(f'Max Reward is {max_reward} occured on episode {max_reward_ep}')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of actions:Box(-1.0, 1.0, (2,), float32), dim of states: 8\n",
            "Episode: 0 Reward: 5.467273642243262 function: 9.019761963531181x^2 + 0.1265116571693259y^2 + 2.2209899730620464xy + -8.535736265067861x + 6.154923114447275y + 1.0659685496996634 x:133180.78337234215 y:133055.60054905762 x_min:-42.971715683237896 y_min:352.8719350494179\n",
            "Episode: 1 Reward: 10.01502138231752 function: 2.354699703173587x^2 + 8.23632630835158y^2 + 4.496243575575045xy + 8.451564000700465x + -4.53967354559625y + 1.9382396724627888 x:-134512.600291092 y:134613.1708149885 x_min:-2.78296537331136 y_min:1.0352044763636958\n",
            "Episode: 2 Reward: 9.604238122464569 function: 0.3188613345962976x^2 + 2.0425449563319775y^2 + 9.577327122601393xy + 8.214806176498477x + 4.763124615230501y + -1.4343980111171781 x:-136279.64227294674 y:136368.50519156537 x_min:-0.13532063797953758 y_min:-0.8487241830622004\n",
            "Episode: 3 Reward: 9.941877769452274 function: 6.095403768537947x^2 + 6.630632126154392y^2 + -5.582220998238466xy + 3.0155511431397173x + 1.588818597766494y + 5.989211056564944 x:135589.26740753694 y:135902.3417496209 x_min:-0.37438725226073255 y_min:-0.27740424357871385\n",
            "Episode: 4 Reward: 8.976875404693358 function: 0.806267064023185x^2 + 1.2179327327166756y^2 + 9.406731407581297xy + 0.9107009493604465x + 9.261153337707498y + 1.0386701881761162 x:-137038.06258060003 y:137232.96550790346 x_min:-1.0040227378432212 y_min:0.0752992670945654\n",
            "Episode: 5 Reward: 8.413882800441709 function: 5.506060756505047x^2 + 0.3077174840338337y^2 + 2.304454661652871xy + -7.101972527594502x + -7.999659138322157y + -6.9666235782264785 x:-137103.71652186965 y:136799.54283312307 x_min:-9.588688647444814 y_min:48.90257939934803\n",
            "Episode: 6 Reward: 8.204964596196687 function: 6.606860851672777x^2 + 6.391054938937307y^2 + -9.446888533268847xy + -3.060722311398685x + -9.826865881937987y + 0.9542155663674734 x:135352.3287601915 y:135238.589019211 x_min:1.656580269567933 y_min:1.9931290904530135\n",
            "Episode: 7 Reward: 21.092894959137613 function: 6.705755730297196x^2 + 1.4383708652212934y^2 + -1.4335296462004266xy + -3.8179377551183302x + 3.085797552152261y + -4.618511069758968 x:134940.52736862068 y:135063.26878562308 x_min:0.17958600003161548 y_min:-0.9831802650664947\n",
            "Episode: 8 Reward: 9.440381508393278 function: 8.373692139701344x^2 + 4.604061667136805y^2 + 5.71774716120829xy + 4.310814293412655x + -4.503568759549125y + -7.835661998604864 x:-134961.80527861373 y:135117.0446002765 x_min:-0.5385543832368179 y_min:0.823499673080335\n",
            "Episode: 9 Reward: 9.29787199454604 function: 4.6824581466034845x^2 + 7.124280280721634y^2 + -2.9719534821308606xy + 5.015087768198114x + -1.4785393101839759y + -1.9862791399069746 x:-134067.57678089553 y:134267.91155565745 x_min:-0.5382138161874689 y_min:-0.008492585229469267\n",
            "Max Reward is 21.092894959137613 occured on episode 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSOngn6BflcJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}